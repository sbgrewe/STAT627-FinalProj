

### Random Forest

```{r}
#| output: false
library(randomForest)
library(tidyverse); theme_set(theme_bw())
reg_data_long <- read_csv(".\\Data\\reg_data_long.csv")
# head(reg_data_long)
reg_data_long$Rating <- as.factor(reg_data_long$Rating)
reg_data_long$female_prop <- reg_data_long$FemalePop/reg_data_long$TotalPop

set.seed(123)
Z <- sample(1:nrow(reg_data_long), 0.8*nrow(reg_data_long), replace = FALSE)
train <- as.data.frame(reg_data_long[Z,])
test <- as.data.frame(reg_data_long[-Z,])
```

```{r}
p <- 9 
RF <- OptimalTrees <- Yhat <- RMSEP <- vector(mode = "double", length = p)

set.seed(123)
for (k in 1:p) {
  RF <- randomForest(DeathsperThou ~ Rating*CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop 
								 + poverty_prop + Rating*prop55older + Rating*HospNumber,
								 data = train, mtry = k)
  OptimalTrees[k] <- which.min(RF$mse)
  RF <- randomForest(DeathsperThou ~ Rating*CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop 
								 + poverty_prop + Rating*prop55older + Rating*HospNumber,
								 data = train, mtry = k, ntree = OptimalTrees[k])
  Yhat <- predict(RF, newdata = test)
  RMSEP[k] <- sqrt(mean((Yhat - test$DeathsperThou)^2) )
}

# which.min(RMSEP)
# OptimalTrees[which.min(RMSEP)]
```

```{r}
#| tbl-cap: "Summary of cross-validated random forest results."
#| label: tbl-rf-results

set.seed(123)
model_RF <- randomForest(DeathsperThou ~ Rating*CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop + poverty_prop + Rating*prop55older + Rating*HospNumber, data = train, mtry = 7, ntree = 398)
# model_RF
# plot(model_RF)

Y_hat <- predict(model_RF, newdata = test)
# mean((test$DeathsperThou-Y_hat)^2)

tibble(
  Metric = c("Number of Predictors", "Number of Trees", "Prediction MSE"),
  Value = list(which.min(RMSEP), OptimalTrees[which.min(RMSEP)], round(mean((test$DeathsperThou-Y_hat)^2), digits = 4))
) %>%  knitr::kable()
```

An exhaustive search using cross validation produced the optimal number of predictors and number of trees to use for the training data: 7 predictors and 398 trees respectively. The random forest model has a prediction MSE of 0.02992, which corresponds to a 37.64% decrease in prediction MSE with respect to the lambda 1se LASSO regression model, and a 44.49% decrease with respect to the best MLR model (@tbl-rf-results). We find that 81.33% of the variation in the heart disease death rate is explained by the combination of 7 variables, and it is again clear that variables beyond case rate are lending predictive power to the model.

We find that hospital rating and amount are significantly used predictors in the random forest, and likely reflect the quality and accessibility of care in different counties ([SI Fig. @fig-rf-imp]). The effect of hospital rating and the number of hospitals far outweigh the effect of demographic factors, including the proportion of residents aged 55 years and older. In general, the random forest model performs with the second lowest prediction error of all regression models explored, and strikes a reasonable balance between bias and complexity in the number of predictors. We fail to find strong evidence of demographic factors, but these regression models illustrate that that hospital quality and number in counties in the state of California are associated with deaths due to heart disease when controlling for variation in the number of heart disease cases.


