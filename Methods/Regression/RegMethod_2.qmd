
```{r}
library(glmnet)
library(GGally)
library(tidyverse); theme_set(theme_bw())
reg_data_long <- read_csv(".\\..\\..\\Data\\reg_data_long.csv")
# head(reg_data_long)
reg_data_long$female_prop <- reg_data_long$FemalePop/reg_data_long$TotalPop

set.seed(123)
Z <- sample(1:nrow(reg_data_long), 0.8*nrow(reg_data_long), replace = FALSE)
train <- as.data.frame(reg_data_long[Z,])
test <- as.data.frame(reg_data_long[-Z,])
```

### Exploration of Multicollinearity

```{r}
reg_data_long[, c("CasesperThou", "prop_hpsa", "nonwhite_prop", "median_income", "unempl_prop","poverty_prop", "prop55older", "HospNumber")]

ggpairs(reg_data_long[, c("CasesperThou", "prop_hpsa", "nonwhite_prop", "median_income", "unempl_prop","poverty_prop", "prop55older", "HospNumber")])

car::vif(lm(DeathsperThou ~ Rating + CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop + poverty_prop + prop55older + HospNumber, data = train))
```

There are a few notable instances of collinearity within the predictors. `nonwhite_prop` and `prop55older` can be reliably predicted from the rest of the data, and . In this analysis, we will rely on variable selection and shrinkage methods to address this multicollinearity.


### LASSO Regression

The goal of this regression is to determine whether deaths per 1000 individuals due to heart disease can be predicted given demographic information, as well as information about the hospitals located within counties in California, controlling for the cases within a county. In order to investigate the relationship between our predictors and the deaths due to heart disease, we employed a 10-fold LASSO regression to determine predictors with non-zero associations with the outcome, even with a shrinkage penalty. 

```{r}
model_train <- lm(DeathsperThou ~ Rating*CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop 
								 + poverty_prop + Rating*prop55older + Rating*HospNumber,
								 data = train)

X_model <- model.matrix(model_train)
X_model <- X_model[,-1]
y_train <- train$DeathsperThou

model_test <- lm(DeathsperThou ~ Rating*CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop 
								 + poverty_prop + Rating*prop55older + Rating*HospNumber,
								 data = test)

X_test <- model.matrix(model_test)
X_test <- X_test[,-1]
y_test <- test$DeathsperThou
```


```{r}
set.seed(123)
LASSO_reg <- cv.glmnet(X_model, y_train, alpha = 1)
cbind(coef(LASSO_reg), coef(LASSO_reg, s=LASSO_reg$lambda.min))
```

```{r}
plot(LASSO_reg)
```

```{r}
y_hat_1s <- predict(LASSO_reg, newx = X_test, type = "response", s = "lambda.1se")
y_hat_min <- predict(LASSO_reg, newx = X_test, type = "response", s = "lambda.min")

c("MSE 1se" = mean((y_hat_1s - y_test)^2), "MSE min" = mean((y_hat_min - y_test)^2))
```

The prediction MSE for the lambda 1se model with one predictors is 0.04798, while the prediction MSE for the lambda min model with seven predictors is 0.01501. The prediction error is expected to decrease by a non-negligible 68.72%. It is clear that these variables are lending predictive power to the model, but not enough to outweigh the L1 regularization penalty. The dominant term is the cases per county.

In the lambda min model, we find that additional predictors of deaths due to heart disease include the median income, hospital ratings, number of hospitals, and proportion of residents 55 years old and older. This agrees with common sense: we can imagine that the overall rate of deaths due to heart disease in a given area is due to 

We find that the rating of a hospital on its own has no statistically significant effect on heart attack deaths until it interacts with the number of heart attack cases. In general, subgroups of hospitals rated "Worse" tended to have higher deaths, while deaths were expected to *decrease overall* for hospitals with "Better" ratings with cases remaining the same. In general, this means that 


