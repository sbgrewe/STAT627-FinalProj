
```{r}
#| echo: false
#| output: false
library(glmnet)
library(GGally)
library(tidyverse); theme_set(theme_bw())
reg_data_long <- read_csv(".\\Data\\reg_data_long.csv")
# head(reg_data_long)
reg_data_long$female_prop <- reg_data_long$FemalePop/reg_data_long$TotalPop

set.seed(123)
Z <- sample(1:nrow(reg_data_long), 0.8*nrow(reg_data_long), replace = FALSE)
train <- as.data.frame(reg_data_long[Z,])
test <- as.data.frame(reg_data_long[-Z,])
```

### Further Exploration of Multicollinearity

In social science and health data, multicollinearity has complex causes and is generally unavoidable. There are a few notable instances of collinearity within the regression predictors ([SI Fig. @fig-reg-multicoll]). The non-white racial population proportion and the population proportion aged 55 years and older can be reliably predicted from the rest of the data, and certain pairs, such as poverty and unemployment, show evidence of an intuitively strong linear association. 

While the cause of these co-correlations is outside of the scope of our analysis, we will now explore two additional regression methods, LASSO and random forest, which use shrinkage or bagging to reduce the variance in the prediction. Unlike simply removing predictors from the analysis, these two methods also allow for unbiased searches across all predictors to construct the best final model.

### LASSO Regression

We employed a 10-fold LASSO regression to determine whether deaths per 1000 individuals due to heart disease can be predicted given demographic information, as well as information about the hospitals located within counties in California, controlling for heart disease cases and with a shrinkage penalty applied to address multicollinearity. 

```{r}
model_train <- lm(DeathsperThou ~ Rating*CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop 
								 + poverty_prop + Rating*prop55older + Rating*HospNumber,
								 data = train)

X_model <- model.matrix(model_train)
X_model <- X_model[,-1]
y_train <- train$DeathsperThou

model_test <- lm(DeathsperThou ~ Rating*CasesperThou + prop_hpsa + nonwhite_prop + median_income + unempl_prop 
								 + poverty_prop + Rating*prop55older + Rating*HospNumber,
								 data = test)

X_test <- model.matrix(model_test)
X_test <- X_test[,-1]
y_test <- test$DeathsperThou
```


```{r}
#| tbl-cap: "Coefficients and estimates obtained after LASSO regression at two different L1 penalty strengths."
set.seed(123)
LASSO_reg <- cv.glmnet(X_model, y_train, alpha = 1)

# plot(LASSO_reg)
# coef(LASSO_reg)
mat <- cbind(coef(LASSO_reg), coef(LASSO_reg, s=LASSO_reg$lambda.min))
summ <- summary(mat)
n <- length(mat[,1])

data.frame(Min    = mat[,2],
           `1se`  = mat[,1]
           ) -> LASSO_estimates_SI
# LASSO_estimates_SI %>% knitr::kable(digits = 4)
```

```{r}
#| tbl-cap: "Overview and prediction performance of the LASSO models at two different L1 penalty strengths."
#| label: tbl-lasso-overview
y_hat_1s <- predict(LASSO_reg, newx = X_test, type = "response", s = "lambda.1se")
y_hat_min <- predict(LASSO_reg, newx = X_test, type = "response", s = "lambda.min")

tibble(
  Model = c("lambda 1se", "lambda min"),
  `Prediction MSE` = c(mean((y_hat_1s - y_test)^2), mean((y_hat_min - y_test)^2)),
  `Number of Predictors` = c(1, 3)
) %>% knitr::kable()
```

The prediction MSE for the lambda 1se model, which had only one significant predictor, is 0.04798, while the prediction MSE for the lambda min model with three predictors is 0.01501 (@tbl-lasso-overview). The sole predictor in the first model was the heart disease case rate, and the difference in prediction error between the two models is a fairly large 68.72%. It is clear that these variables are lending predictive power to the model, but not enough to significantly outweigh a stronger L1 penalty imposed by LASSO. 

In the lambda min model, we find that additional predictors of deaths due to heart disease include the median income, hospital rating, number of hospitals in the county at that rating level, and proportion of residents 55 years old and older ([SI Table @tbl-lasso-ests]). This agrees with common sense: we can imagine that the overall rate of deaths due to heart disease in a given area is due to demographic effects on the health of the general population and economic effects impeding treatment. We also find that the rating of a hospital on its own has no statistically significant effect on heart attack deaths until it interacts with the number of heart attack cases ([SI Table @tbl-lasso-ests]). In general, subgroups of hospitals rated "Worse" tended to have higher deaths, while deaths were expected to *decrease overall* for hospitals with "Better" ratings with cases remaining the same. LASSO demonstrates that there is a trade-off between model simplicity and prediction power, and only in complex models do we see an effect due to demographic and hospital-level variables. 


