```{r}
#| echo: false
#| message: false
library(tidyverse)
hpsa <- read_csv("Data/hpsa.csv")
```

## Method 3: Boosting Method

```{r}
#| warning: false
#| echo: false
library(gbm3)
```

```{r}
#| echo: false
#Convert each character variable to a factor
hpsa2 <- mutate(hpsa, across(where(is.character), as.factor))
```

```{r}
#| echo: false
set.seed(123)
Z <- sample(nrow(hpsa2), nrow(hpsa2)/2)
train <- hpsa2[Z,]
test <- hpsa2[-Z,]
```

### Initial boosting model

```{r}
set.seed(123)
#| echo: false
#| tbl-cap: "Initial boosting model ouput"
#| label: tbl-initialboosting
#| 
boosth <- gbm(hpsa_status ~., data = train, 
              n.trees = 3000, distribution = "Bernoulli")
boosth
```

```{r}
#| fig-cap: "Top 6 important variables for initial boosting model"
#| label: fig-initialboosting
#| fig-width: 5
#| echo: false
summary(boosth, cBars = 6)
```

```{r}
#| echo: false
#To see the tree structure
#<<<<<<< HEAD
#pretty.gbm.tree(boosth)
#pretty.gbm.tree(boosth, i.tree = 3000)
#gbm::pretty.gbm.tree(boosth)
#gbm::pretty.gbm.tree(boosth, i.tree = 3000)
#>>>>>>> c64d14439bbefc0845299f8c30ad009ef9341a50
```

```{r}
yhat <- predict(boosth, newdata = test, n.trees = 3000, type = "response")

# Convert to classification prediction
Yhat_class <-  ifelse(yhat >= 0.5, "Withdrawn", "Designated")
cfm_yhat <- table(Yhat_class, test$hpsa_status)

# prediction error rate and accuracy
yhat_error <- mean(Yhat_class != test$hpsa_status)
accuracy_yhat <- sum(cfm_yhat[1], cfm_yhat[4])/sum(cfm_yhat)
```

### Using Shrinkage parameter

```{r}
set.seed(123)
#| echo: false
#| tbl-cap: "boosting model with shrinkage parameter ouput"
#| label: tbl-boostingshrinkage
#adjust the shrinkage parameter

boosth_shrink <- gbm(hpsa_status ~ ., data = train, 
              n.trees = 5000, shrinkage = 0.001, distribution = "Bernoulli")
boosth_shrink
```

```{r}
#| fig-cap: "Top 6 important variables for boosting with shrinkage model"
#| label: fig-boostingshrinking
#| fig-width: 5
summary(boosth_shrink, cBars = 6)
```

```{r}
#| echo: false
yhat2 <- predict(boosth_shrink, newdata = test, n.trees = 3000, type = "response")

# Convert to classification prediction
Yhat2_class <-  ifelse(yhat2 >= 0.5, "Withdrawn", "Designated")
cfm_yhat2 <- table(Yhat2_class, test$hpsa_status)

# prediction error rate and accuracy
yhat2_error <- mean(Yhat2_class != test$hpsa_status)
accuracy_yhat2 <- sum(cfm_yhat2[1], cfm_yhat2[4])/sum(cfm_yhat2)
```

### Using Cross Validation with shrinkage parameter

```{r}
set.seed(123)

#| echo: false
#| tbl-cap: "Cross validated boosting model with shrinkage parameter"
#| label: tbl-boostingshrinkage-cv

boosth_cv <- gbm(hpsa_status ~ ., data = train, 
              n.trees = 3000, shrinkage = 0.001, 
              cv.folds = 10, 
              distribution = "Bernoulli")
boosth_cv
```

```{r}
#| echo: false
#| fig-cap: "Top 6 important variables for CV boosting_shrinkage model"
#| label: fig-boostingshrinking_cv
#| fig-width: 5
summary(boosth_cv)
```

```{r}

#| fig-cap: "Top 6 important variables for CV boosting_shrinkage model"
#| label: fig-boostingshrinking_cv
#| fig-width: 5

gbm.perf(boosth_cv, method = "cv")
```

Both training deviance (black line) and testing deviance (green line) seemed fairly closed.

```{r}
yhat3 <- predict(boosth_cv, newdata = test, n.trees = 3000, type = "response")

# Convert to classification prediction
Yhat3_class <-  ifelse(yhat3 >= 0.5, "Withdrawn", "Designated")
cfm_yhat3 <- table(Yhat3_class, test$hpsa_status)

# prediction error rate and accuracy
yhat3_error <- mean(Yhat3_class != test$hpsa_status)
accuracy_yhat3 <- sum(cfm_yhat3[1], cfm_yhat3[4])/sum(cfm_yhat3)
```

```{r}
library(gt)

tibble(
  Models = c("initial model", "boost_shrinkage", "boost_shrinkage_cv"),
  Accuracy = c(accuracy_yhat, accuracy_yhat2, accuracy_yhat3),
  Error = c(yhat_error, yhat2_error, yhat3_error)
) |>
  gt() |>
  tab_header(title = "Accuracy and Prediction Error Estimates of Boosting models")
```

### Summary of boosting models

Our analysis shows that increasing or decreasing the shrinkage rate did not affect the performance of the model with shrinkage parameter, and the cross validated model with shrinkage parameter .The cross-validated boosted model with shrinkage parameter @tbl-boostingshrinkage-cv , boosted model with shrinkage parameter @tbl-boostingshrinkage and initial boosted @tbl-initialboosting model all have similar prediction mean squared error.
Top important predictors in the initial @fig-initialboosting and boost_shrinkage @fig-boostingshrinking models were metropolitan indicator and hpsa score.
While the top important predictors in the boost_shrinkage_cv @fig-boostingshrinking_cv model were hpsa designation population, hpsa status and metropolitan indicator.
In all three models, hpsa score and metropolitan indicator were the consistent predictors of hpsa status.
We will recommend the boost_shrinkage_cv model because we believe it will likely generalize to unseen data having used cross validation to tune model parameters.
