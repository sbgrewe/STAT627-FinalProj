```{r}
#| echo: false
#| message: false
library(tidyverse)
hpsa <- read_csv("./Data/hpsa.csv")
```

## Method 3: Boosting Method

```{r}
#| echo: false
library(gbm3)
```

```{r}
#| echo: false
#Convert each character variable to a factor
hpsa2 <- mutate(hpsa, across(where(is.character), as.factor))
```

```{r}
#| echo: false
set.seed(123)
Z <- sample(nrow(hpsa2), nrow(hpsa2)/2)
train <- hpsa2[Z,]
test <- hpsa2[-Z,]
```

### Initial boosting model

```{r}
#| echo: false
set.seed(123)
boosth <- gbm(hpsa_status ~., data = train, 
              n.trees = 3000, distribution = "Bernoulli")
boosth
```

```{r}
#| echo: false
summary(boosth, cBars = 6)
```

```{r}
#To see the tree structure
#pretty.gbm.tree(boosth)
#pretty.gbm.tree(boosth, i.tree = 3000)
```

```{r}
#| include: false
#| layout-ncol: 2

plot(boosth, "metropolitan_indicator")
plot(boosth, "hpsa_score")
#plot(boosth, c("hpsa_population_type", "hpsa_score"))
```

```{r}
yhat <- predict(boosth, newdata = test, n.trees = 3000, type = "response")

# Convert to classification prediction
Yhat_class <-  ifelse(yhat >= 0.5, "Withdrawn", "Designated")
cfm_yhat <- table(Yhat_class, test$hpsa_status)

# prediction error rate and accuracy
yhat_error <- mean(Yhat_class != test$hpsa_status)
accuracy_yhat <- sum(cfm_yhat[1], cfm_yhat[4])/sum(cfm_yhat)
```

### Using Shrinkage parameter

```{r}
#| echo: false
#adjust the shrinkage parameter
set.seed(123)
boosth_shrink <- gbm(hpsa_status ~ ., data = train, 
              n.trees = 5000, shrinkage = 0.001, distribution = "Bernoulli")
summary(boosth_shrink, cBars = 6)
```

```{r}
#| echo: false
yhat2 <- predict(boosth_shrink, newdata = test, n.trees = 3000, type = "response")

# Convert to classification prediction
Yhat2_class <-  ifelse(yhat2 >= 0.5, "Withdrawn", "Designated")
cfm_yhat2 <- table(Yhat2_class, test$hpsa_status)

# prediction error rate and accuracy
yhat2_error <- mean(Yhat2_class != test$hpsa_status)
accuracy_yhat2 <- sum(cfm_yhat2[1], cfm_yhat2[4])/sum(cfm_yhat2)
```

### Using Cross Validation with shrinkage parameter

```{r}
#| echo: false
set.seed(123)
boosth_cv <- gbm(hpsa_status ~ ., data = train, 
              n.trees = 3000, shrinkage = 0.001, 
              cv.folds = 10, 
              distribution = "Bernoulli")
boosth_cv
```

```{r}
#| echo: false
summary(boosth_cv)
```

```{r}
gbm.perf(boosth_cv, method = "cv")
```

```{r}
yhat3 <- predict(boosth_cv, newdata = test, n.trees = 3000, type = "response")

# Convert to classification prediction
Yhat3_class <-  ifelse(yhat3 >= 0.5, "Withdrawn", "Designated")
cfm_yhat3 <- table(Yhat3_class, test$hpsa_status)

# prediction error rate and accuracy
yhat3_error <- mean(Yhat3_class != test$hpsa_status)
accuracy_yhat3 <- sum(cfm_yhat3[1], cfm_yhat3[4])/sum(cfm_yhat3)
```

```{r}
library(gt)

tibble(
  Models = c("initial model", "boost_shrinkage", "boost_shrinkage_cv"),
  Accuracy = c(accuracy_yhat, accuracy_yhat2, accuracy_yhat3),
  Error = c(yhat_error, yhat2_error, yhat3_error)
) |>
  gt() |>
  tab_header(title = "Accuracy and Prediction Error Estimates of Boosting models")
```

### Summary of boosting models

Our analysis that increasing or decreasing the shrinkage rate did not affect the performance of the model with shrinkage parameter, and the cross validated model with shrinkage parameter .The cross-validated boosted model with shrinkage parameter, boosted model with shrinkage parameter and initial boosted model all have similar prediction mean squared error.
Top important predictors in the initial and boost_shrinkage models were metropolitan indicator and hpsa score.
While the top important predictors in the boost_shrinkage_cv model were hpsa designation population, hpsa status and metropolitan indicator.
In all three models, hpsa score and metropolitan indicator were the consistent predictors of hpsa status We will recommend the boost_shrinkage_cv model because we believe it will likely generalize to unseen data.
