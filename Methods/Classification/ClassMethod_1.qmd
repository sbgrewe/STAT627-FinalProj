```{r}
#| echo: true

library(tidyverse)
```

## Loading the dataset

```{r}
#| message: false
hpsa <- read_csv("./Data/hpsa.csv") 
glimpse(hpsa)
```

### Creating training and testing sets

Here we will be using 60% of our dataset for training and 40% for testing

```{r}
set.seed(123)

training_pct <- .50 
Z <- sample(nrow(hpsa), training_pct*nrow(hpsa))

hpsa_train <- hpsa[Z, ]
hpsa_test <- hpsa[-Z, ]

nrow(hpsa_test)
nrow(hpsa_train)
```

### Models

```{r}
set.seed(123)
log_full <- glm(as.factor(hpsa_status) ~., data = hpsa_train, 
                family = "binomial")

summary(log_full)
```

```{r}
set.seed(123)
log_reduced <- glm(as.factor(hpsa_status) ~ designation_type + hpsa_score + 
                     rural_status + hpsa_population_type, data = hpsa_train, 
                   family = "binomial")

summary(log_reduced)
```

### Prediction Accuracy

```{r}

# Reduced model
# Predictions
preds_reduced <- predict(log_reduced, newdata = hpsa_test, type = "response")

# Convert to classification prediction
Yhat_reduced <-  ifelse(preds_reduced >= 0.5, "Withdrawn", "Designated")

# confusion matrix
confm_reduced <- table(Yhat_reduced, hpsa_test$hpsa_status)
confm_reduced

# Correct classification prediction rate
accuracy_reduced <- sum(confm_reduced[1], confm_reduced[4])/sum(confm_reduced)
accuracy_reduced

# test error rate
error_reduced <- mean(Yhat_reduced != hpsa_test$hpsa_status)
error_reduced
```

```{r}

# Full model
# Predictions
preds <- predict(log_full, newdata = hpsa_test, type = "response")

# Convert to classification prediction
Yhat <-  ifelse(preds >= 0.5, "Withdrawn", "Designated")

# confusion matrix
confm <- table(Yhat, hpsa_test$hpsa_status)
confm

# Correct classification prediction rate
accuracy <- sum(confm[1], confm[4])/sum(confm)
accuracy

# test error rate 
error_full <- mean(Yhat != hpsa_test$hpsa_status)
error_full
```

Accuracy of full model was slightly higher than the reduced model, with the full model also having the lowest test error rate.

### Using K-fold cross validation to find prediction mean squared error

```{r}
set.seed(123)

# Loss function
Lossfn <- function(Y, p) {
  mean(1 * (Y == 1 & p <= .50) | (1 * (Y == 0 & p > .50)),
       na.rm = TRUE)
}

# Convert response to numeric 0's and 1's
hpsa$Y <- as.numeric(as.factor(hpsa$hpsa_status)) - 1
```

```{r}
#| warning: false
library(boot)
set.seed(123)

## Prediction error rate
# K = 10

# KFold Reduced
Kfold_reduced <- cv.glm(hpsa, log_reduced, cost = Lossfn, K=10)$delta
Kfold_reduced


# Full Model 
Kfold_full <- cv.glm(hpsa, log_full, cost = Lossfn, K=10)$delta
Kfold_full
```

While the full and reduced model had similar prediction mean squared error, the full model had a slightly lower prediction mean squared error.

### Compare models

```{r}
anova(log_reduced, log_full, test = "Chisq")
```

Giving the results obtained and from this approaches and the results of model comparison we lack sufficient evidence (p = 1.551e-09) to conclude that the reduced logistic regression model with parameters (`designation_type`, `hpsa_score`, `rural status`, and `hpsa_population_type`) is better than the full logistic regression model.
Consequently, we will recommend the full logistic regression model as the better model.
